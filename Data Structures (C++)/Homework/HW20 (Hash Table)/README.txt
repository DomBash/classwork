Hash Tables Project

The purpose of this project was to show our understanding of hash tables and other concepts we have learned in class. We are using two different ways of handling collisions in the hash table so we can better understand the costs of one versus another. We are evaluating the amount of work the computer has to do using different collision solutions so that we can determine which ones would be the most efficient in different situations. This project helps us the practice the things that we have learned in the class and helps us to better understand the cost of a program in relation to creating and traversing data structures. 
	The 5 different algorithms I used in this assignment helped to build the table and to traverse it. 
To build the table I used the hash function, hash sum. The hash sum algorithm takes the input string of the player’s name and turns it into the sum of the ASCII values of its symbols. It then finds the modulus value of this sum to the size of the array you are going to be placing your data into. The modulus of these numbers is the index of the player in the hash array and is returned. After you have the index value from the hash sum algorithm you need to place the player in the array and handle any collisions in the array.
To solve these collisions I used 2 different algorithms to compare which one is the most efficient. The algorithm for solving collisions with a chain starts by checking to see if there is already a player in the hash array at the index location. If there is no player at that index it simply places it there. If there is already a player there it will check to see if the new hash element is the same player and just a different year. If it is just a different year I will add the new year info to a vector that is attached to the player. If it is a new player it will check to see if the next element in the linked list attached to that array index has a player or is the same player as the new one. If it finds that the player already exists in the linked list it will just add the year and team stats. If it reaches the end of the linked list without finding the new player is already there it will add the player node at the end of the linked list.
The other way I resolved collision was by using an open addressing algorithm. When using open addressing I again check to make sure that the array at the player’s index is already taken. If that array element is take it will move to the next hash array element linearly and check to see if there is a player already at that index. If there isn’t a player at that index it the new player will be placed there. If this index is also taken it will continue to increase linearly until it finds an open index to place the new player. If it reaches the end of the array without finding and open index it will wrap around to the first hash array element and continue to search linearly from here for an empty index.
I then use two different algorithms for traversing the open addressing array and the chaining array to find specific players. In the open addressing array, I start by finding the index of the input player with hash sum. Next, I look to see if that player is at that index already. If it’s not at that index I just loop through the hash array linearly while checking to see if the player at each index matches the player that was input. Once the player is found I just print the player’s stats that are stored with it.
	To search for a player in the chaining array I simply check to see if the first node at the player index in the array has the same name as the player being searched for. If not I just loop through the linked list nodes attached to that array index until I find the player being searched for. Once the player node is found I just print the player’s stats. 
To test these algorithms I used tables of different sizes to see how they would be handled. Using the minimum size of 5147 then increasing the size from there to 10,000, 13,000, and 20,000.
The data used was a list of 26,421 professional baseball players from 1985 to 2016 and some of their statistics. The statistics include the player's first and last name, the year they were born, the country they were born in, their weight and height, the arm that they throw with, the arm that they bat with, every year that they played, every team they were on, the state of the team they were on, their league ID, and their salary for every year they played. The first and last name of each player was taken and each letter was turned into its ASCII value and added together and then the modulus operator was used to make sure that the sum would be equal to an index in the hash array. 
The data of each player was stored in a player node in the hash array. In the player node, there are pointers to all of the statistics about the player that never change like name and birth year. There was also a vector in the player node that stores pointers to the player’s information that would change like what team they were on in each year and their salaries. 
The results clearly show that the chaining collision resolution performs better than the open addressing resolution. With a hash table size of 5147, the chaining resolution had 8303 collisions and 57,473 operations. With the same table size, the open addressing resolution had 24,616 collision and 46,134,590 operations. You can tell from this data how big the difference in performance between the open addressing and chaining collision resolutions is.
I think it is pretty obvious that chaining performs better. In open addressing, you have to find an open spot in the array, so especially once the array gets more full it is going to take even more operations to find an empty element. While using a chaining resolution you don’t have to search for an open spot in the whole array you just have to find the end of the chain at that array index. 
This assignment helped me to better understand the material we have been taught by giving me a way to practice multiple different things we have learned. It also helped me to better understand the cost differences between various different ways of creating and traversing data structures. In the future, I will be able to effectively determine the optimal algorithm for the task at hand.  
